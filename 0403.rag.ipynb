{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d1fe25-e832-4a85-8f89-2e6aa055b2f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0. Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd095f72-3b25-44a2-8857-dd5a6a03e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline,\n",
    "                          TrainingArguments)\n",
    "from peft import (LoraConfig,\n",
    "                  PeftModel)\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc719c6-246c-4289-8eb0-68ac48bf582e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d577668-41da-4df3-b6a1-b6d0bca2a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론에 사용할 LLM\n",
    "SLLM_MODEL_ID = 'google/gemma-1.1-2b-it'\n",
    "# Embedding 모델 ID\n",
    "EMBE_MODEL_ID = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "# hugging face access token을 복사하세요.\n",
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b7fbc-8fe7-4642-841b-428c081713e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceBERT 모델 생성\n",
    "embd_model = SentenceTransformer(EMBE_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab20f1-d2ba-46fc-875f-ec5b2ae89f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full chunks 읽어오기\n",
    "full_chunks = []\n",
    "with open(\"data/chunk_db.json\") as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        full_chunks.append(row['chunk'])\n",
    "len(full_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f788b-bcb0-4943-b620-b57105aeb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk embedding index 읽어오기\n",
    "faissindex_file = \"data/faiss_flat_l2.index\"\n",
    "vdb_index = faiss.read_index(faissindex_file)\n",
    "type(vdb_index), vdb_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def9c47-bd90-4132-b8e8-26045ed998de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare 4 bits quantize\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7eed4-6d05-42b7-8e2a-bd4ed9985859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 4 bits model\n",
    "sllm_model = AutoModelForCausalLM.from_pretrained(SLLM_MODEL_ID,\n",
    "                                                  device_map='auto',\n",
    "                                                  quantization_config=quantization_config,\n",
    "                                                  token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b6b6b-cbc0-438c-b368-87efb4edbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "sllm_tokenizer = AutoTokenizer.from_pretrained(SLLM_MODEL_ID,\n",
    "                                          add_special_tokens=True,\n",
    "                                          token=HF_TOKEN)\n",
    "sllm_tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6ce93-145c-4249-8114-280a73ac7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm 추론 pipeline\n",
    "# https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=sllm_model,\n",
    "                tokenizer=sllm_tokenizer,\n",
    "                max_new_tokens=512)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0b85f-65a6-4fbe-a426-44f1c72065be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_context(query, top_n):\n",
    "    query_embedding = embd_model.encode(query, normalize_embeddings=True)\n",
    "    query_embeddings = query_embedding.reshape(1, -1)\n",
    "    D, I = vdb_index.search(query_embeddings, top_n)\n",
    "    context = []\n",
    "    for i in I[0]:\n",
    "        context.append(full_chunks[i])\n",
    "    return '\\n\\n'.join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a567ddd-a208-44f1-990a-d705429ea635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(pipe, context, query):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"당신이 가진 지식을 의존하지 말고 다음 내용을 참고해서 '질문'에 대해서 답변해 주세요.:\n",
    "\n",
    "{}\n",
    "\n",
    "질문: {}\"\"\".format(context, query)\n",
    "        }\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages,\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098b1c6-771a-4066-97bb-2dfc06c05330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_response(pipe, query, top_n=5):\n",
    "    context = gen_context(query, top_n)\n",
    "    prompt = gen_prompt(pipe, context, query)\n",
    "    # print(prompt)\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725eb731-b202-4575-b941-4c8e845f8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    doc = input('질문 > ')\n",
    "    doc = doc.strip()\n",
    "    if len(doc) == 0:\n",
    "        break\n",
    "    result = gen_response(pipe, doc)\n",
    "    print(f'답변 > {result}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
